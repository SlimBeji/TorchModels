{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder (VAE) Cheat Sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Definitions\n",
    "\n",
    "-   **Posterior**: $p(z|x)$ (True latent distribution given data - encoder's target)\n",
    "-   **Likelihood**: $p(x|z)$ (Data generation model - decoder's role)\n",
    "-   **Prior**: $p(z)$ (Assumed latent distribution, typically $\\mathcal{N}(0,I)$)\n",
    "-   **Evidence**: $p(x)$ (Intractable marginal likelihood)\n",
    "-   **Approximate Posterior**: $q_\\phi(z|x) \\approx \\mathcal{N}(\\mu,\\sigma)$ (Encoder's output, learns $\\mu,\\sigma$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective: Minimize KL Divergence\n",
    "\n",
    "**Goal**: Make $q_\\phi(z|x)$ close to true posterior $p(z|x)$\n",
    "\n",
    "We minimize the KL divergence between these distributions:\n",
    "\n",
    "$$\\min_\\phi \\text{KL}(q_\\phi(z|x) \\parallel p(z|x)) = \\mathbb{E}_{q_\\phi(z|x)} \\left[ \\ln \\frac{q_\\phi(z|x)}{p(z|x)} \\right]$$\n",
    "\n",
    "Key points:\n",
    "\n",
    "1. $\\phi$ are the encoder parameters we optimize\n",
    "2. The expectation is taken over $q_\\phi(z|x)$\n",
    "3. Direct computation is intractable (requires $p(z|x)$ which depends on $p(x)$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes' Rule\n",
    "\n",
    "The fundamental relationships between the distributions:\n",
    "\n",
    "1. **Joint Probability Symmetry**:\n",
    "   $$p(x \\cap z) = p(z \\cap x)$$\n",
    "\n",
    "2. **Conditional Probability Definition**:\n",
    "   $$p(x|z)p(z) = p(z|x)p(x)$$\n",
    "\n",
    "3. **Bayes' Theorem** (Posterior Calculation):\n",
    "   $$p(z|x) = \\frac{p(x|z)p(z)}{p(x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivation Steps\n",
    "\n",
    "1. **Initial KL Objective**:\n",
    "   $$\\text{KL}(q_\\phi(z|x) \\parallel p(z|x)) = \\mathbb{E}_{q_\\phi(z|x)} \\left[ \\ln \\frac{q_\\phi(z|x)}{p(z|x)} \\right]$$\n",
    "\n",
    "2. **Apply Bayes' Rule**:\n",
    "   $$= \\mathbb{E}_{q_\\phi(z|x)} \\left[ \\ln \\frac{q_\\phi(z|x)p(x)}{p(x|z)p(z)} \\right]$$\n",
    "\n",
    "3. **Logarithm Expansion**:\n",
    "   $$= \\mathbb{E}_{q_\\phi(z|x)} \\left[ \\ln \\frac{q_\\phi(z|x)}{p(z)} + \\ln p(x) - \\ln p(x|z) \\right]$$\n",
    "   $$= \\underbrace{\\mathbb{E}_{q_\\phi(z|x)} \\left[ \\ln \\frac{q_\\phi(z|x)}{p(z)} \\right]}_{(1)} + \\underbrace{\\mathbb{E}_{q_\\phi(z|x)} [\\ln p(x)]}_{(2)} - \\underbrace{\\mathbb{E}_{q_\\phi(z|x)} [\\ln p(x|z)]}_{(3)}$$\n",
    "\n",
    "4. **Term Analysis**:\n",
    "\n",
    "    - **(1)**: $\\text{KL}(q_\\phi(z|x) \\parallel p(z))$ (Latent Loss)\n",
    "    - **(2)**: $\\ln p(x)$ (Constant, can be ignored for optimization)\n",
    "    - **(3)**: $\\mathbb{E}[\\ln p(x|z)]$ (Reconstruction Loss)\n",
    "\n",
    "5. **Final Loss Components**:\n",
    "   $$\\text{Minimization Target} = \\underbrace{-\\mathbb{E}_{q_\\phi(z|x)} [\\ln p(x|z)]}_{\\text{Reconstruction Loss}} + \\underbrace{\\text{KL}(q_\\phi(z|x) \\parallel p(z))}_{\\text{Latent Loss}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Implementation (PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Loss (KL Divergence)\n",
    "\n",
    "**Closed-form solution for Gaussians** (using [Gaussian KL Identity](https://stats.stackexchange.com/questions/7440/kl-divergence-between-two-univariate-gaussians)):\n",
    "\n",
    "For $q_\\phi(z|x) = \\mathcal{N}(\\mu, \\sigma^2)$, $p(z) = \\mathcal{N}(0, I)$:\n",
    "$$\\text{KL} = \\frac{1}{2} \\sum (\\mu^2 + \\sigma^2 - \\ln \\sigma^2 - 1)$$\n",
    "\n",
    "PyTorch implementation:\n",
    "```python\n",
    "kl_loss = 0.5 * torch.sum(mu.pow(2) + logvar.exp() - logvar - 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction Loss (Negative Log-Likelihood)\n",
    "\n",
    "For image data, we model the decoder output as a Gaussian distribution. The negative log-likelihood simplifies to **Mean Squared Error** (MSE):\n",
    "\n",
    "$$-\\mathbb{E}_{q_\\phi(z|x)} [\\ln p_\\theta(x|z)] \\approx \\|x - \\text{decoder}(z)\\|^2$$\n",
    "\n",
    "PyTorch implementation:\n",
    "\n",
    "```python\n",
    "recon_loss = F.mse_loss(decoder_output, input, reduction='sum')\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
